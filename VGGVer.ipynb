{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGVer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZNQi787akL"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVDG6CTwtYpZ"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uslP2YkO7fHE"
      },
      "source": [
        "\"\"\"\n",
        "Hyperparameters\n",
        "\"\"\"\n",
        "num_epochs = 10\n",
        "batch_size_train = 100\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.005\n",
        "momentum = 0.5\n",
        "log_interval = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g25HAvj2A8DS"
      },
      "source": [
        "\"\"\"\n",
        "Loading data from EMNIST\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.EMNIST('/files/', split='letters', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.RandomPerspective(), \n",
        "                               torchvision.transforms.RandomRotation(10, fill=(0,)), \n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.EMNIST('/files/', split='letters', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74WqGxjpA_Q9"
      },
      "source": [
        "\"\"\"\n",
        "Printing example data. First data is how the processed image will look like\n",
        "\"\"\"\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "\n",
        "print(example_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it-2BaZJDuP6"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 9))\n",
        "ex = example_data[0][0].numpy()\n",
        "ex1 = example_data[1][0].numpy()\n",
        "diff = set()\n",
        "sum = 0\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    if ex[i, j] < 2:\n",
        "      ex[i, j] = -1\n",
        "    else:\n",
        "      ex[i, j] = 1\n",
        "plt.imshow(torch.from_numpy(ex), cmap='gray', interpolation='none')\n",
        "\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyZVMvpiD-9y"
      },
      "source": [
        "class VGG(nn.Module):  \n",
        "    \"\"\"\n",
        "    Based on - https://github.com/kkweon/mnist-competition\n",
        "    from: https://github.com/ranihorev/Kuzushiji_MNIST/blob/master/KujuMNIST.ipynb\n",
        "    \"\"\"\n",
        "    def two_conv_pool(self, in_channels, f1, f2):\n",
        "        s = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(f1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(f2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        for m in s.children():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        return s\n",
        "    \n",
        "    def three_conv_pool(self,in_channels, f1, f2, f3):\n",
        "        s = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(f1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(f2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(f2, f3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(f3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        for m in s.children():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        return s\n",
        "        \n",
        "    \n",
        "    def __init__(self, num_classes=62):\n",
        "        super(VGG, self).__init__()\n",
        "        self.l1 = self.two_conv_pool(1, 64, 64)\n",
        "        self.l2 = self.two_conv_pool(64, 128, 128)\n",
        "        self.l3 = self.three_conv_pool(128, 256, 256, 256)\n",
        "        self.l4 = self.three_conv_pool(256, 256, 256, 256)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTiD7iovEApa"
      },
      "source": [
        "Half_width =128\n",
        "layer_width =128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssMOZ-xdEH58"
      },
      "source": [
        "device = 'cuda' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZOWbkevEJiC"
      },
      "source": [
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYkQLMaEM5F"
      },
      "source": [
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "\n",
        "model = VGG().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHNVZW1eESvE"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWxYYAvIK8c"
      },
      "source": [
        "total_step = len(train_loader)\n",
        "\n",
        "best_accuracy = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        images_np = images.numpy()\n",
        "        for h in range(100):\n",
        "            for k in range(28):\n",
        "              for l in range(28):\n",
        "                if images_np[h, 0, k, l] < 2:\n",
        "                  images_np[h, 0, k, l] = -1\n",
        "                else:\n",
        "                  images_np[h, 0, k, l] = 1\n",
        "        images = torch.from_numpy(images_np).to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i == 499:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "        \n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            images_np = images.numpy()\n",
        "            \"\"\"\n",
        "            Image processing of EMNIST determined by experimentation\n",
        "            \"\"\"\n",
        "            for h in range(100):\n",
        "              for k in range(28):\n",
        "                for l in range(28):\n",
        "                  if images_np[h, 0, k, l] < 2:\n",
        "                    images_np[h, 0, k, l] = -1\n",
        "                  else:\n",
        "                    images_np[h, 0, k, l] = 1\n",
        "            images = torch.from_numpy(images_np).to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "        if best_accuracy>= correct / total:\n",
        "            curr_lr = learning_rate*np.asscalar(pow(np.random.rand(1),3))\n",
        "            update_lr(optimizer, curr_lr)\n",
        "            print('Test Accuracy: {} % Best: {} %'.format(100 * correct / total, 100*best_accuracy))\n",
        "        else:\n",
        "            best_accuracy = correct / total\n",
        "            net_opt = model\n",
        "            print('Test Accuracy: {} % (improvement)'.format(100 * correct / total))\n",
        "\n",
        "        \n",
        "        model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAgu8lXtQRm"
      },
      "source": [
        "Main Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiuVeVa45tsp"
      },
      "source": [
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from scipy import misc\n",
        "import imageio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNteghQztn3U"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5ZHQY79s92"
      },
      "source": [
        "im = Image.open(BytesIO(uploaded['sample_letter2.PNG']))\n",
        "im = im.resize((28, 28))\n",
        "im.save('resized2.PNG')\n",
        "img = imageio.imread('resized2.PNG')\n",
        "img = np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
        "plt.imshow(img, cmap = plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zszl-13H413F"
      },
      "source": [
        "sum = 0\n",
        "max = -100\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    if img[i, j] > max:\n",
        "      max = img[i, j]\n",
        "    sum += img[i, j]\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    img[i, j] = max - img[i, j]\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    img[i, j] = ((img[i, j]) - (max - (sum / 784))) / (max - (sum / 784))\n",
        "\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    if img[i, j] < 1:\n",
        "      img[i, j] = -1\n",
        "    else:\n",
        "      img[i, j] = 1\n",
        "\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    num = 0\n",
        "    if i > 0:\n",
        "      if img[i-1, j] > 0:\n",
        "        num+=1\n",
        "    if i < 25:\n",
        "      if img[i+1, j] > 0:\n",
        "        num+=1\n",
        "    if j > 0:\n",
        "      if img[i, j-1] > 0:\n",
        "        num += 1\n",
        "    if j < 25:\n",
        "      if img[i, j+1] > 0:\n",
        "        num += 1\n",
        "    if num == 0:\n",
        "      img[i, j] = -1\n",
        "    if i == 0 or j == 0 or i == 27 or j == 27 or i == 1 or j == 1 or i == 26 or j == 26:\n",
        "      img[i, j] = -1\n",
        "\n",
        "\n",
        "plt.imshow(img, cmap = plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XPACPBQ_I2D"
      },
      "source": [
        "img = img.astype(float)\n",
        "t = torch.from_numpy(np.array([np.array([img])])).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulWS8EH4BuZ_"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(t.float())\n",
        "  print(torch.max(output.data, 1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}